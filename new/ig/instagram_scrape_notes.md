## Instagram Scraper Overview

1. **Entry Point**: `instagram_scraper.py` (same folder). Run with:
   ```bash
   python3 instagram_scraper.py [--username samjaycomic] [--limit 10] [--all]
   ```
   - `--limit N` fetches the newest `N` posts (default 10).
   - `--all` ignores the limit and keeps paging until the GraphQL endpoint stops.
   - Output JSON path: `./instagram_feed.json`.

2. **Initial Fetch**: Uses `https://www.instagram.com/api/v1/users/web_profile_info/?username=<user>`
   - Headers must include the public IG app id `936619743392459` and a realistic browser UA.
   - Response contains the first ~12 timeline edges plus `page_info.end_cursor` and `has_next_page`.

3. **Pagination Strategy**:
   - Attempted GraphQL query hashes (`69cba40317214236af40e7efa697781d`, `58b6785bea111c67129decbe6a448951`) for bulk paging.
   - Instagram intermittently returns `401`/`Please wait a few minutes…` unless you provide authenticated cookies.
   - The fallback is to re-hit `web_profile_info` with `max_id=<end_cursor>`; it accepts the cursor generated by the previous page.
   - Keep looping while `has_next_page` is `true` and advance `max_id` to the new cursor.

4. **Data Stored per Post**:
   - `id`, `shortcode`, `permalink`, `media_type`, `caption`, like/comment counts, ISO timestamp, and an array of media items (each with `id`, `type`, `url`, `thumbnail_url`, `accessibility_caption`).
   - No media is downloaded; URLs remain hot-linked to Instagram’s CDN.

5. **Throttling Notes**:
   - Rapid paging triggers `require_login` or `Please wait` responses. Pause a few minutes or slow the loop (sleep 1–2s between requests) to avoid soft bans.
   - If Instagram blocks the GraphQL endpoint, continue with the `web_profile_info?max_id=` approach even though it only returns 12 posts per call (will take ~32 requests for 377 posts).
   - For long-term stability consider rotating IP/User-Agent pairs or caching cursors so you can resume later without restarting.

6. **Next Steps**:
   - Add retry/backoff logic to the script before enabling `--all` in automation.
   - Optional: store the last successful cursor in a state file to resume if throttling kicks in mid-run.
